{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install https://github.com/PyThaiNLP/pythainlp/archive/dev.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name=\"train\"\n",
    "import codecs\n",
    "from tqdm import tqdm\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.tag import pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import glob\n",
    "import nltk\n",
    "import re\n",
    "# thai cut\n",
    "thaicut=\"newmm\"\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_validate,train_test_split\n",
    "import pycrfsuite\n",
    "from pythainlp.corpus.common import thai_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(thai_stopwords())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#จัดการประโยคซ้ำ\n",
    "data_not=[]\n",
    "def Unique(p):\n",
    " text=re.sub(\"<[^>]*>\",\"\",p)\n",
    " text=re.sub(\"\\[(.*?)\\]\",\"\",text)\n",
    " text=re.sub(\"\\[\\/(.*?)\\]\",\"\",text)\n",
    " if text not in data_not:\n",
    "  data_not.append(text)\n",
    "  return True\n",
    " else:\n",
    "  return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# เตรียมตัวตัด tag ด้วย re\n",
    "pattern = r'\\[(.*?)\\](.*?)\\[\\/(.*?)\\]'\n",
    "tokenizer = RegexpTokenizer(pattern) # ใช้ nltk.tokenize.RegexpTokenizer เพื่อตัด [TIME]8.00[/TIME] ให้เป็น ('TIME','ไง','TIME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# จัดการกับ tag ที่ไม่ได้ tag\n",
    "def toolner_to_tag(text):\n",
    " text=text.strip().replace(\"FACILITY\",\"LOCATION\").replace(\"[AGO]\",\"\").replace(\"[/AGO]\",\"\").replace(\"[T]\",\"\").replace(\"[/T]\",\"\")\n",
    " text=re.sub(\"<[^>]*>\",\"\",text)\n",
    " text=re.sub(\"(\\[\\/(.*?)\\])\",\"\\\\1***\",text)#.replace('(\\[(.*?)\\])','***\\\\1')# text.replace('>','>***') # ตัดการกับพวกไม่มี tag word\n",
    " text=re.sub(\"(\\[\\w+\\])\",\"***\\\\1\",text)\n",
    " text2=[]\n",
    " for i in text.split('***'):\n",
    "  if \"[\" in i:\n",
    "   text2.append(i)\n",
    "  else:\n",
    "   text2.append(\"[word]\"+i+\"[/word]\")\n",
    " text=\"\".join(text2)#re.sub(\"[word][/word]\",\"\",\"\".join(text2))\n",
    " return text.replace(\"[word][/word]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# แปลง text ให้เป็น conll2002\n",
    "def text2conll2002(text,pos=True):\n",
    "    \"\"\"\n",
    "    ใช้แปลงข้อความให้กลายเป็น conll2002\n",
    "    \"\"\"\n",
    "    text=toolner_to_tag(text)\n",
    "    text=text.replace(\"''\",'\"')\n",
    "    text=text.replace(\"’\",'\"').replace(\"‘\",'\"')#.replace('\"',\"\")\n",
    "    tag=tokenizer.tokenize(text)\n",
    "    j=0\n",
    "    conll2002=\"\"\n",
    "    for tagopen,text,tagclose in tag:\n",
    "        word_cut=word_tokenize(text,engine=thaicut) # ใช้ตัวตัดคำ newmm\n",
    "        i=0\n",
    "        txt5=\"\"\n",
    "        while i<len(word_cut):\n",
    "            if word_cut[i]==\"''\" or word_cut[i]=='\"':pass\n",
    "            elif i==0 and tagopen!='word':\n",
    "                txt5+=word_cut[i]\n",
    "                txt5+='\\t'+'B-'+tagopen\n",
    "            elif tagopen!='word':\n",
    "                txt5+=word_cut[i]\n",
    "                txt5+='\\t'+'I-'+tagopen\n",
    "            else:\n",
    "                txt5+=word_cut[i]\n",
    "                txt5+='\\t'+'O'\n",
    "            txt5+='\\n'\n",
    "            #j+=1\n",
    "            i+=1\n",
    "        conll2002+=txt5\n",
    "    if pos==False:\n",
    "        return conll2002\n",
    "    return postag(conll2002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ใช้สำหรับกำกับ pos tag เพื่อใช้กับ NER\n",
    "def postag(text):\n",
    "    listtxt=[i for i in text.split('\\n') if i!='']\n",
    "    list_word=[]\n",
    "    for data in listtxt:\n",
    "        list_word.append(data.split('\\t')[0])\n",
    "    #print(text)\n",
    "    list_word=pos_tag(list_word,engine=\"perceptron\", corpus=\"orchid_ud\")\n",
    "    text=\"\"\n",
    "    i=0\n",
    "    for data in listtxt:\n",
    "        text+=data.split('\\t')[0]+'\\t'+list_word[i][1]+'\\t'+data.split('\\t')[1]+'\\n'\n",
    "        i+=1\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# เขียนไฟล์ข้อมูล conll2002\n",
    "def write_conll2002(file_name,data):\n",
    "    \"\"\"\n",
    "    ใช้สำหรับเขียนไฟล์\n",
    "    \"\"\"\n",
    "    with codecs.open(file_name, \"w\", \"utf-8-sig\") as temp:\n",
    "        temp.write(data)\n",
    "    return True\n",
    "# อ่านข้อมูลจากไฟล์\n",
    "def get_data(fileopen):\n",
    "\t\"\"\"\n",
    "    สำหรับใช้อ่านทั้งหมดทั้งในไฟล์ทีละรรทัดออกมาเป็น list\n",
    "    \"\"\"\n",
    "\twith codecs.open(fileopen, 'r',encoding='utf-8-sig') as f:\n",
    "\t\tlines = f.read().splitlines()\n",
    "\treturn [a for a in tqdm(lines) if Unique(a)] # เอาไม่ซ้ำกัน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alldata(lists):\n",
    "    text=\"\"\n",
    "    for data in lists:\n",
    "        text+=text2conll2002(data)\n",
    "        text+='\\n'\n",
    "    return text\n",
    "\n",
    "def alldata_list(lists):\n",
    "    data_all=[]\n",
    "    for data in lists:\n",
    "        data_num=[]\n",
    "        try:\n",
    "            txt=text2conll2002(data,pos=True).split('\\n')\n",
    "            for d in txt:\n",
    "                tt=d.split('\\t')\n",
    "                if d!=\"\":\n",
    "                    if len(tt)==3:\n",
    "                        data_num.append((tt[0],tt[1],tt[2]))\n",
    "                    else:\n",
    "                        data_num.append((tt[0],tt[1]))\n",
    "            #print(data_num)\n",
    "            data_all.append(data_num)\n",
    "        except:\n",
    "            print(data)\n",
    "    #print(data_all)\n",
    "    return data_all\n",
    "\n",
    "def alldata_list_str(lists):\n",
    "\tstring=\"\"\n",
    "\tfor data in lists:\n",
    "\t\tstring1=\"\"\n",
    "\t\tfor j in data:\n",
    "\t\t\tstring1+=j[0]+\"\t\"+j[1]+\"\t\"+j[2]+\"\\n\"\n",
    "\t\tstring1+=\"\\n\"\n",
    "\t\tstring+=string1\n",
    "\treturn string\n",
    "\n",
    "def get_data_tag(listd):\n",
    "\tlist_all=[]\n",
    "\tc=[]\n",
    "\tfor i in listd:\n",
    "\t\tif i !='':\n",
    "\t\t\tc.append((i.split(\"\\t\")[0],i.split(\"\\t\")[1],i.split(\"\\t\")[2]))\n",
    "\t\telse:\n",
    "\t\t\tlist_all.append(c)\n",
    "\t\t\tc=[]\n",
    "\treturn list_all\n",
    "def getall(lista):\n",
    "    ll=[]\n",
    "    for i in tqdm(lista):\n",
    "        o=True\n",
    "        for j in ll:\n",
    "            if re.sub(\"\\[(.*?)\\]\",\"\",i)==re.sub(\"\\[(.*?)\\]\",\"\",j):\n",
    "                o=False\n",
    "                break\n",
    "        if o==True:\n",
    "            ll.append(i)\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 5096/5096 [00:00<00:00, 21112.53it/s]\n",
      "100% 5089/5089 [02:06<00:00, 40.30it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data1=getall(get_data(file_name+\".txt\"))\n",
    "print(len(data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 510/510 [00:00<00:00, 10482.57it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val=getall(get_data(\"val.txt\"))\n",
    "print(len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 1274/1274 [00:00<00:00, 8768.36it/s]\n",
      "100% 1274/1274 [00:08<00:00, 153.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test=getall(get_data(\"test.txt\"))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=alldata_list(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data=alldata_list(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=alldata_list(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5089\n",
      "0\n",
      "1274\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isThai(chr):\n",
    " cVal = ord(chr)\n",
    " if(cVal >= 3584 and cVal <= 3711):\n",
    "  return True\n",
    " return False\n",
    "def isThaiWord(word):\n",
    " t=True\n",
    " for i in word:\n",
    "  l=isThai(i)\n",
    "  if l!=True and i!='.':\n",
    "   t=False\n",
    "   break\n",
    " return t\n",
    "\n",
    "def is_stopword(word):\n",
    "    return word in stopwords\n",
    "def is_s(word):\n",
    "    if word == \" \" or word ==\"\\t\" or word==\"\":\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def lennum(word,num):\n",
    "    if len(word)==num:\n",
    "        return True\n",
    "    return False\n",
    "def doc2features(doc, i):\n",
    "    word = doc[i][0]\n",
    "    postag = doc[i][1]\n",
    "    # Features from current word\n",
    "    features={\n",
    "        'word.word': word,\n",
    "        'word.stopword': is_stopword(word),\n",
    "        'word.isthai':isThaiWord(word),\n",
    "        'word.isspace':word.isspace(),\n",
    "        'postag':postag,\n",
    "        'word.isdigit()': word.isdigit()\n",
    "    }\n",
    "    if word.isdigit() and len(word)==5:\n",
    "        features['word.islen5']=True\n",
    "    if i > 0:\n",
    "        prevword = doc[i-1][0]\n",
    "        postag1 = doc[i-1][1]\n",
    "        features['word.prevword'] = prevword\n",
    "        features['word.previsspace']=prevword.isspace()\n",
    "        features['word.previsthai']=isThaiWord(prevword)\n",
    "        features['word.prevstopword']=is_stopword(prevword)\n",
    "        features['word.prepostag'] = postag1\n",
    "        features['word.prevwordisdigit'] = prevword.isdigit()\n",
    "    else:\n",
    "        features['BOS'] = True # Special \"Beginning of Sequence\" tag\n",
    "    # Features from next word\n",
    "    if i < len(doc)-1:\n",
    "        nextword = doc[i+1][0]\n",
    "        postag1 = doc[i+1][1]\n",
    "        features['word.nextword'] = nextword\n",
    "        features['word.nextisspace']=nextword.isspace()\n",
    "        features['word.nextpostag'] = postag1\n",
    "        features['word.nextisthai']=isThaiWord(nextword)\n",
    "        features['word.nextstopword']=is_stopword(nextword)\n",
    "        features['word.nextwordisdigit'] = nextword.isdigit()\n",
    "    else:\n",
    "        features['EOS'] = True # Special \"End of Sequence\" tag\n",
    "    return features\n",
    "\n",
    "def extract_features(doc):\n",
    "    return [doc2features(doc, i) for i in range(len(doc))]\n",
    "\n",
    "def get_labels(doc):\n",
    "    return [tag for (token,postag,tag) in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 5089/5089 [00:08<00:00, 569.42it/s]\n",
      "100% 5089/5089 [00:00<00:00, 121009.89it/s]\n"
     ]
    }
   ],
   "source": [
    "X_data = [extract_features(doc) for doc in tqdm(train_data)]\n",
    "y_data = [get_labels(doc) for doc in tqdm(train_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.15 s, sys: 71.9 ms, total: 3.23 s\n",
      "Wall time: 3.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "i=0\n",
    "for xseq, yseq in zip(X_data, y_data):\n",
    "    try:\n",
    "      trainer.append(xseq, yseq)\n",
    "      i+=1\n",
    "    except:\n",
    "      pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "    'c1': 0.1,   # coefficient for L1 penalty\n",
    "    'c2': 0.1,  # coefficient for L2 penalty\n",
    "    'max_iterations': 500,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature.minfreq',\n",
       " 'feature.possible_states',\n",
       " 'feature.possible_transitions',\n",
       " 'c1',\n",
       " 'c2',\n",
       " 'max_iterations',\n",
       " 'num_memories',\n",
       " 'epsilon',\n",
       " 'period',\n",
       " 'delta',\n",
       " 'linesearch',\n",
       " 'max_linesearch']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.train('thai-ner-1-5-newmm-orchid_ud.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [extract_features(doc) for doc in tqdm(test_data)]\n",
    "y_test = [get_labels(doc) for doc in tqdm(test_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from itertools import chain\n",
    "from sklearn.metrics import classification_report\n",
    "def bio_classification_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Classification report for a list of BIO-encoded sequences.\n",
    "    It computes token-level metrics and discards \"O\" labels.\n",
    "    \n",
    "    Note that it requires scikit-learn 0.15+ (or a version from github master)\n",
    "    to calculate averages properly!\n",
    "    \"\"\"\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('thai-ner-1-5-newmm-orchid_ud.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(tagger.info().labels.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.remove('O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_pred = [tagger.tag(xseq) for xseq in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(tagger.info().labels.keys())\n",
    "labels.remove('O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bio_classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv thai-ner-1-5-newmm-orchid_ud.crfsuite thainer_crf_1_5_2-orchid_ud.model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
